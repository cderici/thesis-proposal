\subsection{Tracing JITs \& RPython Framework}
\label{subsec:rpython}

JIT compilers allow programs to be compiled dynamically at run-time
during the evaluation via an interpreter. As opposed to the
ahead-of-time (AOT) compilers, JITs interleave the compilation with
execution, aiming to compile and optimize only the parts of the
program that are used the most. JIT compilers employ a low-overhead
profiling to dynamically detect a frequent (i.e. hot) sequence of
instructions during interpretation. Starting with evaluating a given
program with an interpreter, a JIT compiler stops the evaluation when
a frequently evaluated instruction path is detected, compiles the
instructions and starts using the compiled path whenever the same path
needs to be evaluated again. [CITE?]

  \begin{wrapfigure}[23]{r}{0.35\textwidth}
    %\vspace{-0.5cm}
    \centering
    \begin{minipage}[t]{0.32\textwidth}
      \begin{minted}[numbersep=0pt,gobble=0,linenos,numbers=right,fontsize=\scriptsize,frame=lines]{python}
# start of the trace (outer loop)
label(p0, p1, descr="64723312")
guard_not_invalidated()
guard_class(p0, ConsEnv)
p3 = getfield_gc_r(p0, descr="ConsEnv.prev")
guard_class(p3, ConsEnv)
i5 = getfield_gc_i(p3, descr="Fixnum")
i6 = getfield_gc_i(p0, descr="Fixnum")
i7 = int_add_ovf(i5, i6)
guard_no_overflow()
guard_class(p1, LetCont)
p9 = getfield_gc_r(p1, descr="LetCont.ast")
guard_value(p9, ConstPtr(ptr10))
p11 = getfield_gc_r(p1, descr="LetCont.env")
p12 = getfield_gc_r(p1, descr="LetCont.prev")
# inner loop
label(p11, i7, p12, descr="64723392")
guard_not_invalidated()
guard_class(p11, ConsEnv)
i14 = getfield_gc_i(p11, descr="Fixnum")
i15 = int_add_ovf(i14, i7)
guard_no_overflow()
guard_class(p12, LetCont)
p17 = getfield_gc_r(p12, descr="LetCont.ast")
guard_value(p17, ConstPtr(ptr18))
p19 = getfield_gc_r(p12, descr="LetCont.env")
p20 = getfield_gc_r(p12, descr="LetCont.prev")
# jump
jump(p19, i15, p20, descr="64723392")

    \end{minted}
    \end{minipage}
    \caption{\small An example trace}
    \label{fig:trace}
  \end{wrapfigure}

JITs have been shown to be significantly effective in implementing
efficient VMs for dynamic languages [CITE]. There are two main
approaches in JIT compilation. Method-based compilation work on the
method level, and compile the most frequently used methods in the
program [CITE], while trace-based JIT compilers compile the most
frequently evaluated loops [CITE]. In this thesis, we're interested in
the trace-based JIT compilation, as we work with meta-tracing,

Trace-based JIT compilation works on two principal assumptions:

\textbf{i)} Programs spend most of their running time in loops.

\textbf{ii)} Several iterations of the same loop are likely to take
similar code paths.

To exploit these assumptions, a tracing JIT classifies certain
execution paths to be \emph{hot loops} during the evaluation. When a
hot loop is detected, as described before the evaluation stops, and
loop is compiled and optimized into a \emph{trace}, and then the
execution continues, using the compiled trace whenever the same path
needs to be executed. A trace is a linear sequence of instructions
with an entry point and one or more exit points. \figref{fig:trace}
shows a simplified example of a typical trace, taking two arguments
$p0$ and $p1$ as inputs, having a preamble (because of unrolling) and
an inner loop that jumps to itself. The \emph{guards} within a trace
make up the trace's exit points, where the trace jumps out if certain
conditions are not satisfied. They are often used for \textbf{i)}
making sure in the preamble the conditions are the same when this
sequence is first traced, and \textbf{ii)} to finish and exit the
loop.

  \begin{wrapfigure}[20]{r}{0.35\textwidth}
    %\vspace{-0.5cm}
    \centering
    \begin{minipage}[t]{0.32\textwidth}
      \begin{minted}[numbersep=0pt,gobble=0,linenos,numbers=right,fontsize=\scriptsize,frame=lines]{python}
driver = JitDriver(greens = ['pc', 'bytecode'],
                   reds   = ['a', 'regs'])

def interpret(bcode, a):
  regs = [0] * 256
  pc = 0

  while True:
    driver.jit_merge_point(bcode=bytecode,
                          pc=pc,a=a,
                          regs=regs)
    opcode = ord(bytecode[pc])
    pc += 1

    if opcode == JUMP_IF_A:
      target = ord(bcode[pc])
      pc += 1
      if a:
        if target < pc:
          driver.can_enter_jit(bytecode=bcode,
                              pc=target,
                              a=a, regs=regs)
        pc = target
    elif opcode == MOV_A_R:
      ...
    \end{minted}
    \end{minipage}
    \caption{\small An interpreter annotated with hints. Figure
      adapted from \cite{bolz09}}
    \label{fig:rpython-interp}
  \end{wrapfigure}

\paragraph{Meta-tracing}
Evaluating a byte-code interpreter for a language in place of a
regular program on a tracing JIT naturally forms an implementation of
a language. However, without any treatment the loops that are traced
during the execuion of the interpreter not necessarily correspond to
the loops implemented in the user program that is being
interpreted. In particular, since a byte-code interpreter is basically
a dispatch loop for the byte-code instructions in the language, a
trace of an execution path within such an interpretation loop consists
of instructions implementing only one particular byte-code. However,
it is rarely the case where a single byte-code is repeatedly executed
when evaluating a program on an interpreter. Additionally the loops in
the user program, in which most of the running time will be spent are
never being traced. Therefore this approach leads to compiling traces
that most likely will be used only once at a time (exit out at the end
of the trace, as the next byte-code will not be the same), resulting
in a worse performance than even just interpreting.

Meta-tracing introduces certain hints and annotations for the tracer
to be utilized within the executed program (in our case the
interpreter) to capture the useful traces that will be executed
repeatedly, e.g. in the case of an interpreter the ones that will
correspond to the loops in the user program. The loop detection is
simply finding a spot where the program jumped
backwards. \figref{fig:rpython-interp} shows an example interpreter
decorated with such annotations and hints. The annotations allow the
implementer to set the parameters (\emph{greens}) that constitute what
can be considered as a compund program counter (pc), by which the
tracer will detect that the program jumped backwards. Additionally the
hints allow the points where the program might jump backwards to be
stated.  The reader is referred to \cite{bolz09} for a detailed
discussion.

\paragraph{RPython framework}

RPython (Restricted Python) is a statically typed, object-oriented
proper subset of Python that is designed during PyPy's development
\cite{pypy06}. It restricts Python in a way that enables
type-inference on RPython programs allowing an efficient conversion to
a lower language such as C. The restrictions are mainly the dynamic
features of Python and commonly listed as \cite{rpython07,rpython09}:

- No mix types at the same location (variables including function arguments need to be type consistent.

- Runtime reflection is not supported (i.e. changing method of classes at run-time).

- No closures.

- Global and class bindings are assumed to be constants.

- Types of all variables in the code must be inferable.


%% \begin{itemize}
%% \item No mix types at the same location (variables including function arguments need to be type consistent.
%% \item Runtime reflection is not supported (i.e. changing method of classes at run-time).
%% \item No closures.
%% \item Global and class bindings are assumed to be constants.
%% \item Types of all variables in the code must be inferable.
%% \end{itemize}

The RPython framework takes RPython code and converts it to a chosen
lower-level language, most commonly C. Given a VM (interpreter)
written in RPython, the toolchain translates it to the target language
by lowering it in numerous phases, where each phase has its own type
system and a generic type inference engine. Because the RPython is a
subset of Python, the entire process can use a Python run-time. The
general translation process can be described as follows. It starts
with loading the program into the run-time and get the function
objects in memory as inputs. Using these function objects, the
framework generates by abstract intepretation the control flow graphs
for these functions that will be processed at further transformation
steps. Then the annotator acts as a high-level type inference engine
and assigns ``annotations'' to each variable at each flow graph. These
annotations basically denotes the possible Python objects a variable
might contain at the run-time. After the whole program is annotated,
RTyper (RPython typer) takes control and starts lowering the
annotations and some operations into the lower types and operations
that would make sense to the targeted environment (e.g. structs,
pointers, arrays in case of C), acting as a bridge between the
high-level annotator and the low-level code generation. After RTyping,
some optional backend optimizations are applied, such as inlining,
malloc removal and escape analysis. Note that like Python, RPython is
garbage collected, however, C is not. Therefore at this point a
garbage collector is inserted into the program as well. Finally the
typed and lowered flow-graphs are inputted into the code generation
and the binary is generated. \cite{rpython07, pypy06, pypy08}
