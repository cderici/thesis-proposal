\newpage

\subsection{Performance Issues with Self-Hosting on JIT Compilers}
\label{subsec:performance}

With many micro and macro benchmarks and larger experiments, the
existing Pycket implementation is shown to be significantly performant
in evaluating Racket code. It benefits from the the existing generic
optimizations in the RPython framework, including common subexpression
elimination, copy propogation, constant folding, loop invariant code
motion, malloc-removal and the inlining that naturally comes for free
from tracing \cite{loop-aware:12, hotpath:06, malloc-removal:11}, as
well as from many improvements on the interpreter such as environment
pruning, data structure specialization, strategies, as well as some
gradual typing related improvements such as the use of
hidden-classes. However, it is also shown that in some cases Pycket is
significantly slower than all the other systems, on "almost
exclusively recursive programs with data-dependent control flow in the
style of an interpreter over an AST" \cite{pycket15, pycket17}. In
this section we identify the key points of these issues and propose
solution approaches that will be essential in implementing efficient
run-times for self-hosting functional langauges on meta-tracing JIT
compilers.

Tracing interpreter-style programs with complex control-flow paths is
a known weakness of JIT compilers. The large number of branches not
only cripple the JIT optimizations but also causes the traces to be
highly data driven. For example, consider the following program, which
is a highly simplified version of the \racketcode{fasl->s-exp}
function of the FASL library in Racket, modified specifically to
demonstrate the control-flow problem.

\begin{wrapfigure}[18]{l}{0.5\textwidth}
\small
\begin{lstlisting}[mathescape]
(define (branchy lst)
  (letrec
    ([loop
      (lambda (lst)
        (let ([index (if (null? lst) 3 (car lst))])
          (if (null? lst)
              -1
              (if (< index 3)
                  (if (< index 2)
                      (loop (cdr lst))
                      (loop (cdr lst)))
                  (if (< index 5)
                      (if (< index 4)
                          (+ 3 (loop (cdr lst)))
                          (+ 5 (loop (cdr lst))))
                      (if (< index 6)
                          (loop (cdr lst))
                          (+ 1 (loop (cdr lst)))))))))])
   (loop lst)))
\end{lstlisting}
\end{wrapfigure}

Tracing this program running with an input, say \racketcode{'(1 5 3)},
produces a trace that follows the control-flow path of the program for
that input, making the tracing quite wasteful because for any other
input, say \racketcode{'(4 7 1)} which follows an entirely different
path on the program, the JIT produces, compiles and optimizes yet
another trace for that input, unable to use the previously generated
trace. This problem not only increases the warmup time but also
produces traces that are unlikely to be frequently re-used, since the
input data itself is not necessarily repetitive.

This problem is immediately observable on Pycket self-hosting Racket
through the expander linklet, because the interpreter style programs
with complex control-flow paths are quite central in self-hosting a
language (e.g. expand, fasl etc.). Additionally, since the Pycket's
level of language abstraction is increased one step further, the
generated traces are much larger, which creates a pressure for the
JIT's compiler and optimizer. As a result, in the run-time Pycket
spends a lot of time compiling and optimizing traces, but often bails
out and interprets the code instead of using the traces, which defeats
the purpose of using a tracing JIT.

Another major actor in this play is the loop invariant code motion
optimization performed by the JIT \cite{loop-aware:12}. This
optimization prepends a trace to itself, moving all the loop-invariant
operations (usually the operations for destructuring the interpreter
state) into the preamble, keeping all the essential operations in the
peeled-iteration (the inner loop). An example of this can be seen in
the trace in \figref{fig:trace} in \secref{subsec:rpython}. Any trace
or a side-trace (i.e. a \emph{bridge}) that is able to jump to the
peeled-iteration of another trace therefore saves time by not
performing the loop-invariant operations. In order to do so, however,
the program state needs to be consistent with the one that is expected
by the peeled-iteration.

The program state consists of heap allocated objects
(e.g. environment, continuation), the virtuals (malloc-removed parts
of the interpreter state), and other loop information such as range
values. In order to jump to a peeled-iteration, the state needs to
match with the one that the iteration is expecting, e.g. the same
state at the end of its preamble. On the other hand, jumping to the
preamble of a trace requires the allocation of all the unallocated
parts of the interpreter state. If a bridge that is produced for a
side-exit is used to jump back to the original trace, the JIT tries to
make it jump to the peeled-iteration whenever possible to avoid
performing the loop-invariant operations. However, this often fails on
Pycket because the interpreter state often can't match the expected
state, because the state is changed differently by different control
flow paths (e.g. non-tail calls add a continuation frame). Note that
on Pycket the major parts in the interpreter state is composed by the
environment and the continuation, which are heap allocated objects.

This issue is partly addressed in the previous versions of Pycket by
allowing the JIT to allocate a little bit more to force a match
between the states. The malloc-removal and escape analysis in the
trace optimizer often allows the JIT to remove parts of state and
create virtuals for the objects that never escape the loop
\cite{malloc-removal:11, loop-aware:12}. The introduced heuristic on
Pycket allows the JIT to allocate such objects elided by the
optimizations, trading some space for jumping into the inner loop to
avoid performing loop-invariant operations. It is shown that with this
heuristic the performance in gradual typing is increased by 4\% on
Pycket, while adding no extra overhead \cite{pycket17}.

Adding the linklets layer to the front-end allows Pycket to run large
Racket programs including the expander. To give a perspective, the
offline generated expander linklet that is processed in Pycket at boot
itself is roughly 5MB file containing the s-expressions of 2642
functions. Another example is, to run a single Racket program written
in the \emph{racket/base} language (such as repl for instance), more
than 100 Racket modules need to be expanded and instantiated first
just to load the language before running the program itself. While
Pycket's performance on the micro-benchmarks is not effected by the
linklet layer (since nothing has changed in the back-end), these large
programs that are now runnable on Pycket aggravate these issues
observed before.

Additionally, as we described in \secref{subsec:pycket}, given a
Racket module, Pycket now first runs the expander to fully expand the
module before running it. Since the expansion of a given Racket module
is included in Pycket's run-time, the JIT suffers from a large number
of heap allocations for the continuation chains often caused by the
control indirections within the macro-system. This, in turn, creates a
pressure for the garbage collection, as the GC is triggered by
allocations.

\subsubsection{Meta-Tracing Complex Control Flows}

%% As discussed in \secref{subsec:rpython}, a trace is generated in a
%% meta-tracing framework by accompanying an interpreter while it is
%% evaluating a program and detecting hot-loops. During this process, the
%% tracer exactly follows all the operations in the interpreter. For
%% example, if a function application is being evaluated, the next
%% operation to be interpreted will be the first operation in the
%% function's body\footnote{This is why inlining comes naturally from
%%   tracing.}. While this approach works quite well in generating
%% re-usable and concise traces for programs/loops with a straightforward
%% control flow thanks to the hints and annotations in meta-tracing
%% \cite{bolz09}, it becomes significantly harder when the control flow
%% gets more complicated. Consider the following program being
%% meta-traced:

Here's the ideal trace we would like to see independent of the input: ....

Here's how we can solve the issue of tracing branchy code, meta-hints
etc. (regexp stuff)

- show how to make the regexp implementation faster and explain.


\subsubsection{Garbage Collection (GC) Pressure}

As briefly described in \secref{subsec:rpython}, the flow graphs and
the C code that are generated during the translation in the RPython
framework assume automatic memory management. Therefore the produced
program is linked with a GC that is implemented within the framework
(in RPython), by inspecting all the graphs and turning all
\texttt{malloc} operations into calls to the GC. RPython's garbage
collector, namely the \emph{minimark GC} is a three-generations
semispace copying generational collector. Each semispace has a nursery
where the young\footnote{The age of an object is the number of
  collections they survived.}  objects are inserted first,
i.e. allocations fill the nursery. And when the nursery is full then
it is collected and the objects that are still alive are moved into
the non-nursery part of the current semispace. This decreases the
times that a full collection is needed. The approach is based on the
idea that the lifetime of the objects that are created are short, and
the amount of live objects that are used by the program fits in the
nursery (set to be the size of CPU Level 2 cache). \cite{pypy06,
  bolz:14, gc:16, gc:12}

Now that the Racket's expander is used to expand a given module within
the Pycket's run-time, the life in the heap is much different than
what a generational GC would like to see. The allocated objects are
rather big and live very long. Even the expander itself is allocated
as a linklet instance containing more than 2000 variables, and live
until the expansion of the given Racket module as well as its language
(entailing the expansion of many Racket modules) is completed. Another
example is the long continuation chains caused by the large depth of
the real-world Racket programs such as the macro-system or the
module-system. The \emph{minimark GC} moves the old and/or big objects
out of the semispace ("external"), where they will not be moved and
collected in a mark-and-sweep fashion to avoid costly moves. However,
since our setup creates many objects (often large) that live long, not
only we benefit the nursery approach enough but also the
mark-and-sweep creates a pressure on the run-time performance.

EXAMPLE


Here's how introducing another interpreter that uses the stack might
help with the heap (therefore solving the GC pressure issue).

- typeset the judgment rules for stackful interpeter

- preliminary experiments, where CEK+stackful performs better than
just CEK
