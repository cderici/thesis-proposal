\newpage

\subsection{Performance Issues with Self-Hosting on JIT Compilers}
\label{subsec:performance}

With many micro and macro benchmarks and larger experiments, the
existing Pycket implementation is shown to be significantly performant
in evaluating Racket code. It benefits from the the existing generic
optimizations in the RPython framework, including common subexpression
elimination, copy propogation, constant folding, loop invariant code
motion, malloc-removal and the inlining that naturally comes for free
from tracing \cite{loop-aware:12, hotpath:06, malloc-removal:11}, as
well as from many improvements on the interpreter such as environment
pruning, data structure specialization, strategies, as well as some
gradual typing related improvements such as the use of
hidden-classes. However, it is also shown that in some cases Pycket is
significantly slower than all the other systems, on "almost
exclusively recursive programs with data-dependent control flow in the
style of an interpreter over an AST" \cite{pycket15, pycket17}. In
this section we identify the key points of these issues and propose
solution approaches that will be essential in implementing efficient
run-times for self-hosting functional langauges on meta-tracing JIT
compilers.

Tracing interpreter-style programs with complex control-flow paths is
a known weakness of JIT compilers. The large number of indirections
not only cripple the JIT optimizations but also causes the loops to be
segmented into a large number of highly data driven traces. For
example, consider the following program in \figref{fig:regexp}, which
is a very simple regular expression matcher. It is highly simplified
and some of the rules are removed for space.

\begin{figure}[h!]
  %\vspace{-0.5cm}
  \footnotesize
  \begin{mdframed}
\begin{lstlisting}[mathescape,escapechar=\%,language=lisp]
a 	Matches the specified character literal          q       q
* 	Matches 0 or more of the previous character      a*      "", a, aa, aaa
. 	Matches any character literal                    .       a, b, c, d, e ...
^ 	Matches the start of a string                    ^c      c, ca, caa, cbb ...

(define (match-star pattern p-pos str s-pos star-pos m)
  (or (and (< s-pos (string-length str)) (char=? (string-ref pattern p-pos) (string-ref str s-pos))
            (%\underline{match-pat}% pattern p-pos str (add1 s-pos) (cons (string-ref str s-pos) m)))
       (%\underline{match-pat}% pattern (add1 star-pos) str s-pos m)))

(define (match-pat pattern p-pos str s-pos m)
  (cond
    [(>= p-pos (string-length pattern)) m]
    [(and (< (+ p-pos 1) (string-length pattern)) (char=? (string-ref pattern (+ p-pos 1)) #\*))
      (%\underline{match-star}% pattern p-pos str s-pos (+ p-pos 1) m)]
    [(char=? (string-ref pattern p-pos) #\.)
      (and (< s-pos (string-length str)) (%\underline{match-pat}% pattern (add1 p-pos) str (add1 s-pos) (cons (string-ref str s-pos) m)))]
    [else
     (and (char=? (string-ref pattern p-pos) (string-ref str s-pos))
           (%\underline{match-pat}% pattern (add1 p-pos) str (add1 s-pos) (cons (string-ref str s-pos) m)))]))

(define (reg-match pattern str)
  (let ([m (cond
             [(char=? (string-ref pattern 0) #\^) (match-pat pattern 1 str 0 '())]
             [(string=? str "") (match-pat pattern 0 str 0 '())] ; edge case
             [else (for/or ([i (in-range (string-length str))])
               (match-pat pattern 0 str i '()))])])
    (and m (list (list->string (reverse m))))))
\end{lstlisting}
\end{mdframed}
\caption{A simple regular expression matcher (some cases are removed for space).}
\label{fig:regexp}
\end{figure}

Tracing this program running with an input regexp, say
$\mathtt{\#}$\racketcode{rx"defg"}, trying to match it against an
input string produces a trace that follows the control-flow path of
the program for that input, making tracing quite wasteful because for
any other input regexp, say $\mathtt{\#}$\racketcode{rx"a*"} which
follows an entirely different path on the program, the JIT produces,
compiles and optimizes yet another trace for that input, unable to use
the previously generated trace. This problem not only increases the
warmup time but also produces traces that are unlikely to be
frequently re-used.

This problem is immediately observable on Pycket self-hosting Racket
through the expander linklet, because the interpreter style programs
with complex control-flow paths are quite central in self-hosting a
language (e.g. expand, fasl etc.). Additionally, since the Pycket's
level of language abstraction is increased one step further, the
generated traces are much larger, which creates a pressure for the
JIT's compiler and optimizer. As a result, in the run-time the JIT
spends a lot of time compiling and optimizing traces, but often bails
out and interprets the code instead of using the traces, which defeats
the purpose of using a tracing JIT.

%% The major part of the problem is about loop-detection. While the loop
%% detection is in general quite central in tracing JITs, for Pycket it
%% is a bit more tricky, as Pycket works on a higher level of abstraction
%% (program AST) than a lower-level bytecode interpreter, and the only
%% way to create a loop is through a function application. Moreover, the
%% self-hosting adds yet another level of abstraction on top, making the
%% loop detection in the source-level even more difficult.

Another major actor in this play is the loop invariant code motion
optimization performed by the JIT \cite{loop-aware:12}. This
optimization prepends a trace to itself, moving all the loop-invariant
operations (usually the operations for destructuring the interpreter
state) into the preamble, keeping all the essential operations in the
peeled-iteration (the inner loop). An example of this can be seen in
the trace in \figref{fig:trace} in \secref{subsec:rpython}. Any trace
or a side-trace (i.e. a \emph{bridge}) that is able to jump to the
peeled-iteration of another trace therefore saves time by not
performing the loop-invariant operations. In order to do so, however,
the program state needs to be consistent with the one that is expected
by the peeled-iteration.

The program state consists of heap allocated objects
(e.g. environment, continuation), the virtuals (malloc-removed parts
of the interpreter state), and other loop information such as range
values. In order to jump to a peeled-iteration, the state needs to
match with the one that the iteration is expecting, e.g. the same
state at the end of its preamble. On the other hand, jumping to the
preamble of a trace requires the allocation of all the unallocated
parts of the interpreter state. If a bridge that is produced for a
side-exit is used to jump back to the original trace, the JIT tries to
make it jump to the peeled-iteration whenever possible to avoid
performing the loop-invariant operations. However, this often fails on
Pycket because the interpreter state often can't match the expected
state, because the state is changed differently by different control
flow paths (e.g. non-tail calls add a continuation frame). Note that
on Pycket the major parts in the interpreter state is composed by the
environment and the continuation, which are heap allocated objects.

This issue is partly addressed in the previous versions of Pycket by
allowing the JIT to allocate a little bit more to force a match
between the states. The malloc-removal and escape analysis in the
trace optimizer often allows the JIT to remove parts of state and
create virtuals for the objects that never escape the loop
\cite{malloc-removal:11, loop-aware:12}. The introduced heuristic on
Pycket allows the JIT to allocate such objects elided by the
optimizations, trading some space for jumping into the inner loop to
avoid performing loop-invariant operations. It is shown that with this
heuristic the performance in gradual typing is increased by 4\% on
Pycket, while adding no extra overhead \cite{pycket17}.

Adding the linklets layer to the front-end allows Pycket to run large
Racket programs including the expander. To give a perspective, the
offline generated expander linklet that is processed in Pycket at boot
itself is roughly 5MB file containing the s-expressions of 2642
functions. Another example is, to run a single Racket program written
in the \emph{racket/base} language (such as repl for instance), more
than 100 Racket modules need to be expanded and instantiated first
just to load the language before running the program itself. While
Pycket's performance on the micro-benchmarks is not effected by the
linklet layer (since nothing has changed in the back-end), these large
programs that are now runnable on Pycket aggravate these issues
observed before.

%% As discussed in \secref{subsec:rpython}, a trace is generated in a
%% meta-tracing framework by accompanying an interpreter while it is
%% evaluating a program and detecting hot-loops. During this process, the
%% tracer exactly follows all the operations in the interpreter. For
%% example, if a function application is being evaluated, the next
%% operation to be interpreted will be the first operation in the
%% function's body\footnote{This is why inlining comes naturally from
%%   tracing.}. While this approach works quite well in generating
%% re-usable and concise traces for programs/loops with a straightforward
%% control flow thanks to the hints and annotations in meta-tracing
%% \cite{bolz09}, it becomes significantly harder when the control flow
%% gets more complicated. Consider the following program being
%% meta-traced:

To understand the issue better, it is important to first see how the
loop detection currently works on Pycket. While for the low-level
languages such as in a bytecode interpreter a program counter is used
to detect loops, in Pycket the focus is on function applications,
since Pycket works on program ASTs and the only AST that may create
loops is an application. As reported in the previous studies, Pycket
utilizes two techniques, namely the two-state tracking and the use of
a dynamic call-graph. In both techniques the idea is to eliminate the
"false-loops" among all the observed trace headers (i.e. potential
start of a loop). The two-state tracking encodes the trace headers as
a pair of a lambda body and its call-site, and the call-graph method
detects cycles on a dynamically generated call-graph to handle extra
levels of indirection. These approaches together are proven to be very
effective in tracing code with a heavy use of shared control-flow
indirections, such as the contract system \cite{pycket15,pycket17}.

The reason that we chose the regular expression matcher as an example
in \figref{fig:regexp} is that Pycket already has an efficient regexp
implementation in RPython to compare against.

The ideal trace that we would like to see for the program in
\figref{fig:branchy} would consists of a single sequence of side-exits
(guards) for all the conditions to dispatch into the corresponding
bridges for each branch, each of which will eventually jump back to
the original loop. However with the current methods on Pycket, the
loop is segmented into multiple traces where all the codes for the
branches are inlined and each trace is specialized to a one particular
path driven by the input data.

Here's how we can solve the issue of tracing branchy code, meta-hints
etc. (regexp stuff)

- show how to make the regexp implementation faster and explain.

\subsubsection{Garbage Collection (GC) Pressure}

The second issue that we identified as one of the essential problems
in self-hosting Racket on meta-traced CEK is the GC pressure caused by
the long chains of continuation allocations on the heap. As we
described in \secref{subsec:pycket}, given a Racket module, Pycket now
first runs the expander to fully expand the module before running
it. Since the expansion of a given Racket module is included in
Pycket's run-time, the JIT suffers from a large number of heap
allocations for the continuation chains often caused by the control
indirections within the macro-system. This, in turn, creates a
pressure for the garbage collection, as the GC is triggered by
allocations. To understand the issue better, we start by briefly
describing the GC utilized by the RPython framework.

As described in \secref{subsec:rpython}, the flow graphs and the C
code that are generated during the translation in the RPython
framework assume automatic memory management. Therefore the produced
program is linked with a GC that is implemented within the framework
(in RPython), by inspecting all the graphs and turning all
\texttt{malloc} operations into calls to the GC. RPython's garbage
collector, namely the \emph{minimark GC} is a three-generations
semispace copying generational collector. Each semispace has a nursery
where the young\footnote{The age of an object is the number of
  collections they survived.}  objects are inserted first,
i.e. allocations fill the nursery. And when the nursery is full then
it is collected and the objects that are still alive are moved into
the non-nursery part of the current semispace. This decreases the
times that a full collection is needed. The approach is based on the
idea that the lifetime of the objects that are created are short, and
the amount of live objects that are used by the program fits in the
nursery (set to be the size of CPU Level 2 cache). \cite{pypy06,
  bolz:14, gc:16, gc:12}

Now that the Racket's expander is used to expand a given module within
the Pycket's run-time, the life in the heap is much different than
what a generational GC would like to see. The allocated objects are
rather big and live very long. Even the expander itself is allocated
as a linklet instance containing more than 2000 variables, and live
during the whole run-time. Another example is the long continuation
chains caused by the large depth of the real-world Racket programs
such as the macro-system or the module-system. In the \emph{minimark
  GC} the old and/or big objects are moved out of the semispace
("external"), where they will not be moved anymore and collected in a
mark-and-sweep fashion to avoid costly moves. However, since our setup
creates many objects (often large) that live long, not only we don't
benefit the nursery approach enough but also the mark-and-sweep
process creates a pressure on the run-time performance, as it is a
serial pasuing collector.

To address this issue (without modifying the underlying framework nor
the GC), we propose to use a stackful interpreter to decrease the
continuation allocations on the heap. The stackful interpreter here is
a simple recursive interpreter for evaluating Racket forms, which is
translated by the RPython framework into a recursive C program that
uses the native stack. Recall that, however, the Pycket's interpreter
is based on the CEK machine, and the heap allocated explicit
continuations makes it very convenient to implement complex control
operations with continuations (as well as the proper handling of
tail-calls). Therefore the proposed approach is to use a stackful
interpreter \emph{alongside} the CEK, rather than replacing the
current CEK with a stackful interpreter. The idea is to use the stack
and heap in balance to decrease the GC pressure and take advantage of
using the stack, such as rapid allocations, locality etc.

Using the stack to implement a language like Racket, however,
understandibly introduces some challenges regarding the stack
management, such as handling overflows due to arbitrarily deep
recursion. Additionally, implementing a stackful interpreter on Pyket
for the entire Racket would be illogical, since it would not only
require implementing everything from scratch including the
continuations, but also not benefit from the current CEK's
performance. Therefore the logical approach is to use the two
interpreters together, repeatedly switch between the two whenever
appropriate, making sure that the switches don't induce a high
performance overhead.

One of the most fundamental points here is the interaction between the
two interpreters. Essentially, we want to use the stack to save some
heap space (thereby reducing the GC pressure) and take advantage of
faster allocations and locality. On the other hand, however, we don't
want to lose the benefits of the JIT's performance optimizations on
the CEK. Moreover, the stack management needs to be on point to avoid
problems such as switching back to the CEK while the stack is very
deep (e.g. stack overflow), since that would require allocating enough
continuations on the heap to counterbalance the performance benefit we
get from using the stackful interpreter in the first place.

To understand this problem better and to work on a simpler setup, I
developed a formalism that includes both the CEK machine and the
stackful interpreter evaluating a very simple subset of Racket
together. [FIXME: Figure asdasd] shows the language we're working on, which
is similar to the Racket Core language used in
\secref{subsec:linklets-semantics} minus the forms for the linklet
variables, plus additional continuations for the CEK and
\racketcode{convert} forms for controlling the interpreter switches
from the source level. [FIXME: Figure cek-switch-stack] shows the overview of
the interaction between the two models. We can start with either of
the interpreters and switch back and forth between them using the
\racketcode{convert} forms.

\begin{itemize}
\item Example switch from cek to stackful
\item Example switch from stackful to cek
\end{itemize}

[FIXME: The bound on the stack and the heap]

[FIXME: The implementation]

\begin{itemize}
\item trampoline
\item switch on overflow
\item primitives
\item switch on higher-order calls
\item entry is still cek
\end{itemize}

[FIXME: Experiment]

\begin{wrapfigure}[19]{l}{0.5\textwidth}
  \vspace{-0.5cm}
\small
\begin{lstlisting}[mathescape]
(define (branchy lst)
  (letrec
    ([loop
      (lambda (lst)
        (let ([index (if (null? lst) 3 (car lst))])
          (if (null? lst)
              -1
              (if (< index 3)
                  (if (< index 2)
                      (loop (cdr lst))
                      (loop (cdr lst)))
                  (if (< index 5)
                      (if (< index 4)
                          (+ 3 (loop (cdr lst)))
                          (+ 5 (loop (cdr lst))))
                      (if (< index 6)
                          (loop (cdr lst))
                          (+ 1 (loop (cdr lst)))))))))])
   (loop lst)))
\end{lstlisting}
\caption{A simple program with non-trivial control flow.}
\label{fig:branchy}
\end{wrapfigure}
