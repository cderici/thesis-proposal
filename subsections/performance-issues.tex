\subsection{Performance Issues with Self-Hosting on JIT Compilers}
\label{subsec:performance}

With many micro-benchmarks and larger experiments, the existing Pycket
implementation is shown to be significantly performant in evaluating
Racket code. It benefits from the the existing generic optimizations
in the RPython framework, including common subexpression elimination,
copy propogation, constant folding, loop invariant code motion,
malloc-removal and the inlining that naturally comes for free from
tracing \cite{loop-aware:12, hotpath:06, malloc-removal:11}, as well
as from many improvements on the interpreter such as environment
pruning, data structure specialization, strategies, as well as some
gradual typing related improvements such the use of hidden-classes
\cite{pycket15, pycket17}.

Adding the linklets layer to the front-end of the existing
implementation doesn't have any effect on the benchmark performance,
since the linklets layer only adds the capability of processing and
running linklets and doesn't include or change anything in the
Pycket's back-end. However, having the linklets allows Pycket to run
the programs that are much larger than micro-benchmarks or individual
experiment programs. To give a perspective, the offline generated
expander linklet that is processed in Pycket at boot itself is roughly
5MB file containing the s-expressions of 2642 functions. Another
example is, to run a single Racket program written in the
\emph{racket/base} language (such as repl for instance), more than 100
Racket modules need to be expanded and instantiated first just to load
the language before running the program itself.

In addition to the problem of being have to evaluate larger programs,
self-hosting also reveals an issue that is fundamental to the tracing
JIT compilers, which is being have to evaluate large dispatch-loops
that create complex control flows that are significantly hard for
tracing. In this section we study both of these problems and attempt
to understand the key parts to address them in the next section.

\subsubsection{Garbage Collection (GC) Pressure}

As briefly described in \secref{subsec:rpython}, the flow graphs and
the C code that are generated during the translation in the RPython
framework assume automatic memory management. Therefore the produced
program is linked with a GC that is implemented within the framework
(in RPython), by inspecting all the graphs and turning all
\texttt{malloc} operations into calls to the GC. RPython's garbage
collector, namely the \emph{minimark GC} is a three-generations
semispace copying generational collector. Each semispace has a nursery
where the young\footnote{The age of an object is the number of
  collections they survived.}  objects are inserted first,
i.e. allocations fill the nursery. And when the nursery is full then
it is collected and the objects that are still alive are moved into
the non-nursery part of the current semispace. This decreases the
times that a full collection is needed. The approach is based on the
idea that the lifetime of the objects that are created are short, and
the amount of live objects that are used by the program fits in the
nursery (set to be the size of CPU Level 2 cache). \cite{pypy06,
  bolz:14, gc:16, gc:12}

Now that the Racket's expander is used to expand a given module within
the Pycket's run-time, the life in the heap is much different than
what a generational GC would like to see. The allocated objects are
rather big and live very long. Even the expander itself is allocated
as a linklet instance containing more than 2000 variables, and live
until the expansion of the given Racket module as well as its language
(entailing the expansion of many Racket modules) is completed. Another
example is the long continuation chains caused by the large depth of
the real-world Racket programs such as the macro-system or the
module-system. The \emph{minimark GC} moves the old and/or big objects
out of the semispace ("external"), where they will not be moved and
collected in a mark-and-sweep fashion to avoid costly moves. However,
since our setup creates many objects (often large) that live long, not
only we benefit the nursery approach enough but also the
mark-and-sweep creates a pressure on the run-time performance.

EXAMPLE

\subsubsection{Meta-Tracing Complex Control Flows}

As discussed in \secref{subsec:rpython}, a trace is generated in a
meta-tracing framework by accompanying an interpreter while it is
evaluating a program and detecting hot-loops. During this process, the
tracer exactly follows all the operations in the interpreter. For
example, if a function application is being evaluated, the next
operation to be interpreted will be the first operation in the
function's body\footnote{This is why inlining comes naturally from
  tracing.}. While this approach works quite well in generating
re-usable and concise traces for programs/loops with a straightforward
control flow thanks to the hints and annotations in meta-tracing
\cite{bolz09}, it becomes significantly harder when the control flow
gets more complicated. Consider the following program:

\begin{lstlisting}[mathescape]
[show branchy code]
\end{lstlisting}
